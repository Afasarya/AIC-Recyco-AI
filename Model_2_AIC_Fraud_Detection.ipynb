{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "82aAETPTrCCn"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import pickle\n",
        "\n",
        "class InventoryVerificationSystem:\n",
        "    def __init__(self):\n",
        "        self.feature_extractor = self._build_feature_extractor()\n",
        "        self.similarity_threshold = 0.85\n",
        "        self.fraud_detection_model = self._build_fraud_detector()\n",
        "\n",
        "    def _build_feature_extractor(self):\n",
        "        \"\"\"CNN untuk extract features dari gambar sampah\"\"\"\n",
        "        base_model = tf.keras.applications.EfficientNetB0(\n",
        "            weights='imagenet',\n",
        "            include_top=False,\n",
        "            input_shape=(224, 224, 3)\n",
        "        )\n",
        "        base_model.trainable = False\n",
        "\n",
        "        model = tf.keras.Sequential([\n",
        "            base_model,\n",
        "            layers.GlobalAveragePooling2D(),\n",
        "            layers.Dense(512, activation='relu'),\n",
        "            layers.Dropout(0.3),\n",
        "            layers.Dense(256, activation='relu'),\n",
        "            layers.L2Normalize()  # Untuk similarity comparison\n",
        "        ])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def _build_fraud_detector(self):\n",
        "        \"\"\"Model untuk detect fraud patterns\"\"\"\n",
        "        inputs = tf.keras.Input(shape=(10,))  # Features: similarity, time, location, etc.\n",
        "\n",
        "        x = layers.Dense(128, activation='relu')(inputs)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Dropout(0.4)(x)\n",
        "\n",
        "        x = layers.Dense(64, activation='relu')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Dropout(0.3)(x)\n",
        "\n",
        "        x = layers.Dense(32, activation='relu')(x)\n",
        "        outputs = layers.Dense(1, activation='sigmoid')(x)  # Fraud probability\n",
        "\n",
        "        model = Model(inputs, outputs)\n",
        "        model.compile(\n",
        "            optimizer='adam',\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy', 'precision', 'recall']\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def extract_features(self, image):\n",
        "        \"\"\"Extract features dari gambar\"\"\"\n",
        "        if isinstance(image, str):\n",
        "            image = cv2.imread(image)\n",
        "\n",
        "        # Preprocessing\n",
        "        image = cv2.resize(image, (224, 224))\n",
        "        image = image.astype('float32') / 255.0\n",
        "        image = np.expand_dims(image, axis=0)\n",
        "\n",
        "        return self.feature_extractor(image)\n",
        "\n",
        "    def calculate_similarity(self, features1, features2):\n",
        "        \"\"\"Hitung cosine similarity\"\"\"\n",
        "        return tf.keras.utils.cosine_similarity(features1, features2).numpy()[0]\n",
        "\n",
        "    def detect_fraud(self, inventory_image, collected_image, metadata):\n",
        "        \"\"\"Main fraud detection function\"\"\"\n",
        "        # Extract features\n",
        "        inv_features = self.extract_features(inventory_image)\n",
        "        col_features = self.extract_features(collected_image)\n",
        "\n",
        "        # Calculate similarity\n",
        "        similarity = self.calculate_similarity(inv_features, col_features)\n",
        "\n",
        "        # Prepare features for fraud detection\n",
        "        fraud_features = np.array([[\n",
        "            similarity,\n",
        "            metadata.get('time_diff', 0),  # Waktu antara scan dan pickup\n",
        "            metadata.get('location_diff', 0),  # Jarak lokasi\n",
        "            metadata.get('user_history_score', 1.0),  # User reputation\n",
        "            metadata.get('waste_type_match', 1.0),  # Type consistency\n",
        "            metadata.get('quantity_ratio', 1.0),  # Quantity consistency\n",
        "            metadata.get('lighting_score', 1.0),  # Image quality\n",
        "            metadata.get('angle_consistency', 1.0),  # Angle similarity\n",
        "            metadata.get('background_match', 1.0),  # Background consistency\n",
        "            metadata.get('timestamp_validity', 1.0)  # Timestamp validation\n",
        "        ]])\n",
        "\n",
        "        # Predict fraud probability\n",
        "        fraud_prob = self.fraud_detection_model.predict(fraud_features)[0][0]\n",
        "\n",
        "        return {\n",
        "            'is_valid': similarity > self.similarity_threshold and fraud_prob < 0.3,\n",
        "            'similarity_score': float(similarity),\n",
        "            'fraud_probability': float(fraud_prob),\n",
        "            'confidence': 1 - abs(0.5 - fraud_prob) * 2,\n",
        "            'details': {\n",
        "                'visual_match': similarity > self.similarity_threshold,\n",
        "                'fraud_risk': 'low' if fraud_prob < 0.3 else 'medium' if fraud_prob < 0.7 else 'high'\n",
        "            }\n",
        "        }\n",
        "\n",
        "# Training function\n",
        "def train_fraud_detector(model, training_data):\n",
        "    \"\"\"Train fraud detection model\"\"\"\n",
        "    X_train, y_train = training_data\n",
        "\n",
        "    # Class weights untuk handle imbalanced data\n",
        "    class_weights = {0: 1, 1: 10}  # Fraud cases lebih sedikit\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=100,\n",
        "        batch_size=32,\n",
        "        validation_split=0.2,\n",
        "        class_weight=class_weights,\n",
        "        callbacks=[\n",
        "            tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(patience=5)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import pandas as pd\n",
        "\n",
        "class WasteDataPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.augmentation = A.Compose([\n",
        "            A.RandomRotate90(p=0.3),\n",
        "            A.HorizontalFlip(p=0.3),\n",
        "            A.RandomBrightnessContrast(p=0.3),\n",
        "            A.GaussNoise(p=0.2),\n",
        "            A.Blur(blur_limit=3, p=0.2),\n",
        "            A.ColorJitter(p=0.3),\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "    def create_fraud_dataset(self, legitimate_pairs, fraud_scenarios):\n",
        "        \"\"\"Generate training dataset untuk fraud detection\"\"\"\n",
        "        data = []\n",
        "\n",
        "        # Legitimate cases\n",
        "        for inv_img, col_img, metadata in legitimate_pairs:\n",
        "            features = self.extract_all_features(inv_img, col_img, metadata)\n",
        "            data.append(features + [0])  # Label: Not fraud\n",
        "\n",
        "        # Fraud cases (synthetic + real if available)\n",
        "        fraud_cases = self.generate_fraud_cases(legitimate_pairs)\n",
        "        for features in fraud_cases:\n",
        "            data.append(features + [1])  # Label: Fraud\n",
        "\n",
        "        df = pd.DataFrame(data, columns=[\n",
        "            'similarity', 'time_diff', 'location_diff', 'user_score',\n",
        "            'waste_type_match', 'quantity_ratio', 'lighting_score',\n",
        "            'angle_consistency', 'background_match', 'timestamp_validity', 'is_fraud'\n",
        "        ])\n",
        "\n",
        "        return df\n",
        "\n",
        "    def generate_fraud_cases(self, legitimate_pairs):\n",
        "        \"\"\"Generate synthetic fraud scenarios\"\"\"\n",
        "        fraud_cases = []\n",
        "\n",
        "        for inv_img, col_img, metadata in legitimate_pairs:\n",
        "            # Scenario 1: Different waste item\n",
        "            fraud_metadata = metadata.copy()\n",
        "            fraud_metadata['similarity'] = np.random.uniform(0.1, 0.4)\n",
        "            fraud_metadata['waste_type_match'] = 0.2\n",
        "            fraud_cases.append(self.extract_all_features(inv_img, col_img, fraud_metadata))\n",
        "\n",
        "            # Scenario 2: Suspicious timing\n",
        "            fraud_metadata = metadata.copy()\n",
        "            fraud_metadata['time_diff'] = np.random.uniform(24, 168)  # 1-7 days\n",
        "            fraud_metadata['user_score'] = 0.3\n",
        "            fraud_cases.append(self.extract_all_features(inv_img, col_img, fraud_metadata))\n",
        "\n",
        "            # Scenario 3: Location mismatch\n",
        "            fraud_metadata = metadata.copy()\n",
        "            fraud_metadata['location_diff'] = np.random.uniform(5, 50)  # 5-50km\n",
        "            fraud_cases.append(self.extract_all_features(inv_img, col_img, fraud_metadata))\n",
        "\n",
        "        return fraud_cases"
      ],
      "metadata": {
        "id": "P7SF4TkdrQwp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "class ModelEvaluator:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def comprehensive_evaluation(self, test_data, test_labels):\n",
        "        \"\"\"Comprehensive model evaluation\"\"\"\n",
        "        predictions = self.model.predict(test_data)\n",
        "        pred_binary = (predictions > 0.5).astype(int)\n",
        "\n",
        "        # Metrics\n",
        "        accuracy = accuracy_score(test_labels, pred_binary)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(test_labels, pred_binary, average='binary')\n",
        "        auc_score = roc_auc_score(test_labels, predictions)\n",
        "\n",
        "        # Confusion Matrix\n",
        "        cm = confusion_matrix(test_labels, pred_binary)\n",
        "\n",
        "        # Calculate business metrics\n",
        "        false_positives = cm[0][1]  # Legitimate flagged as fraud\n",
        "        false_negatives = cm[1][0]  # Fraud missed\n",
        "\n",
        "        results = {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1,\n",
        "            'auc_score': auc_score,\n",
        "            'false_positive_rate': false_positives / (cm[0][0] + cm[0][1]),\n",
        "            'false_negative_rate': false_negatives / (cm[1][0] + cm[1][1]),\n",
        "            'business_impact': {\n",
        "                'users_incorrectly_flagged': false_positives,\n",
        "                'fraud_cases_missed': false_negatives,\n",
        "                'estimated_loss_prevented': false_negatives * 100  # Assume $100 per fraud\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def plot_performance(self, history, test_results):\n",
        "        \"\"\"Visualize model performance\"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "        # Training history\n",
        "        axes[0,0].plot(history.history['accuracy'], label='Train')\n",
        "        axes[0,0].plot(history.history['val_accuracy'], label='Validation')\n",
        "        axes[0,0].set_title('Model Accuracy')\n",
        "        axes[0,0].legend()\n",
        "\n",
        "        # Loss\n",
        "        axes[0,1].plot(history.history['loss'], label='Train')\n",
        "        axes[0,1].plot(history.history['val_loss'], label='Validation')\n",
        "        axes[0,1].set_title('Model Loss')\n",
        "        axes[0,1].legend()\n",
        "\n",
        "        # Confusion Matrix\n",
        "        sns.heatmap(test_results['confusion_matrix'], annot=True, ax=axes[1,0])\n",
        "        axes[1,0].set_title('Confusion Matrix')\n",
        "\n",
        "        # Metrics bar chart\n",
        "        metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']\n",
        "        values = [test_results[m.lower().replace('-', '_')] for m in metrics]\n",
        "        axes[1,1].bar(metrics, values)\n",
        "        axes[1,1].set_title('Performance Metrics')\n",
        "        axes[1,1].set_ylim(0, 1)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "O4oJ2BVvrXVO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== TRAINING SCRIPT YANG DIPERBAIKI =====\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class ImprovedDataGenerator:\n",
        "    def __init__(self):\n",
        "        self.noise_factor = 0.1\n",
        "\n",
        "    def create_realistic_dataset(self, num_samples=10000):\n",
        "        \"\"\"Generate more realistic and noisy dataset\"\"\"\n",
        "        print(\"🔄 Generating realistic training data with noise...\")\n",
        "\n",
        "        np.random.seed(42)  # Untuk reproducibility\n",
        "        data = []\n",
        "\n",
        "        # Legitimate cases (60% of data) - dengan variasi realistis\n",
        "        for i in range(int(num_samples * 0.6)):\n",
        "            # Base legitimate pattern dengan noise\n",
        "            base_similarity = np.random.beta(9, 2)  # Mostly high, some variation\n",
        "            noise = np.random.normal(0, 0.05)\n",
        "            similarity = np.clip(base_similarity + noise, 0.3, 1.0)\n",
        "\n",
        "            features = [\n",
        "                similarity,\n",
        "                np.random.gamma(2, 1.5),           # time_diff: realistic distribution\n",
        "                np.random.gamma(1, 0.8),           # location_diff\n",
        "                np.random.beta(7, 3),              # user_score with variation\n",
        "                np.random.choice([0.7, 0.8, 0.9, 1.0], p=[0.1, 0.2, 0.3, 0.4]),\n",
        "                np.random.normal(1.0, 0.15),       # quantity with noise\n",
        "                np.random.beta(6, 4),              # lighting variations\n",
        "                np.random.beta(7, 3),              # angle consistency\n",
        "                np.random.beta(5, 5),              # background match varies\n",
        "                np.random.choice([0.8, 0.9, 1.0], p=[0.2, 0.3, 0.5]),\n",
        "                0  # legitimate\n",
        "            ]\n",
        "\n",
        "            # Add realistic constraints and noise\n",
        "            features[1] = max(0.5, min(48, features[1]))  # 0.5-48 hours\n",
        "            features[2] = max(0.1, min(10, features[2]))  # 0.1-10 km\n",
        "            features[5] = max(0.7, min(1.3, features[5]))  # quantity ratio\n",
        "\n",
        "            data.append(features)\n",
        "\n",
        "        # Edge cases - legitimate but suspicious (15%)\n",
        "        for i in range(int(num_samples * 0.15)):\n",
        "            features = [\n",
        "                np.random.uniform(0.65, 0.85),     # Lower similarity but still ok\n",
        "                np.random.uniform(6, 24),          # Longer time but acceptable\n",
        "                np.random.uniform(1, 5),           # Further distance\n",
        "                np.random.uniform(0.4, 0.7),       # Lower user score\n",
        "                np.random.choice([0.6, 0.7, 0.8], p=[0.3, 0.4, 0.3]),\n",
        "                np.random.uniform(0.8, 1.2),\n",
        "                np.random.uniform(0.4, 0.8),\n",
        "                np.random.uniform(0.5, 0.8),\n",
        "                np.random.uniform(0.3, 0.7),\n",
        "                np.random.uniform(0.6, 1.0),\n",
        "                0  # still legitimate\n",
        "            ]\n",
        "            data.append(features)\n",
        "\n",
        "        # Clear fraud cases (20%)\n",
        "        for i in range(int(num_samples * 0.2)):\n",
        "            fraud_type = np.random.choice(['visual', 'temporal', 'behavioral', 'mixed'])\n",
        "\n",
        "            if fraud_type == 'visual':\n",
        "                features = [\n",
        "                    np.random.uniform(0.1, 0.45),   # Very low similarity\n",
        "                    np.random.gamma(2, 1),\n",
        "                    np.random.gamma(1, 1),\n",
        "                    np.random.beta(4, 6),\n",
        "                    np.random.uniform(0.2, 0.5),    # Poor type match\n",
        "                    np.random.uniform(0.5, 1.5),\n",
        "                    np.random.uniform(0.3, 0.7),\n",
        "                    np.random.uniform(0.2, 0.6),\n",
        "                    np.random.uniform(0.2, 0.5),\n",
        "                    np.random.uniform(0.5, 0.9),\n",
        "                    1\n",
        "                ]\n",
        "            elif fraud_type == 'temporal':\n",
        "                features = [\n",
        "                    np.random.uniform(0.4, 0.7),\n",
        "                    np.random.uniform(48, 336),     # 2-14 days\n",
        "                    np.random.uniform(2, 20),\n",
        "                    np.random.beta(2, 8),           # Very poor user score\n",
        "                    np.random.uniform(0.4, 0.8),\n",
        "                    np.random.uniform(0.7, 1.3),\n",
        "                    np.random.uniform(0.3, 0.7),\n",
        "                    np.random.uniform(0.4, 0.8),\n",
        "                    np.random.uniform(0.3, 0.7),\n",
        "                    np.random.uniform(0.1, 0.6),    # Poor timestamp validity\n",
        "                    1\n",
        "                ]\n",
        "            elif fraud_type == 'behavioral':\n",
        "                features = [\n",
        "                    np.random.uniform(0.5, 0.8),\n",
        "                    np.random.uniform(1, 12),\n",
        "                    np.random.uniform(10, 100),     # Very far location\n",
        "                    np.random.beta(1, 9),           # Terrible user score\n",
        "                    np.random.uniform(0.3, 0.7),\n",
        "                    np.random.uniform(0.3, 2.0),    # Suspicious quantity\n",
        "                    np.random.uniform(0.2, 0.6),\n",
        "                    np.random.uniform(0.3, 0.7),\n",
        "                    np.random.uniform(0.1, 0.4),    # Very poor background\n",
        "                    np.random.uniform(0.2, 0.7),\n",
        "                    1\n",
        "                ]\n",
        "            else:  # mixed fraud\n",
        "                features = [\n",
        "                    np.random.uniform(0.2, 0.6),\n",
        "                    np.random.uniform(12, 168),\n",
        "                    np.random.uniform(3, 30),\n",
        "                    np.random.beta(2, 8),\n",
        "                    np.random.uniform(0.2, 0.6),\n",
        "                    np.random.uniform(0.4, 1.8),\n",
        "                    np.random.uniform(0.2, 0.6),\n",
        "                    np.random.uniform(0.2, 0.6),\n",
        "                    np.random.uniform(0.1, 0.5),\n",
        "                    np.random.uniform(0.1, 0.6),\n",
        "                    1\n",
        "                ]\n",
        "\n",
        "            data.append(features)\n",
        "\n",
        "        # Ambiguous cases - hard to classify (5%)\n",
        "        for i in range(int(num_samples * 0.05)):\n",
        "            # These should be challenging for the model\n",
        "            label = np.random.choice([0, 1])  # Random label for ambiguous\n",
        "            features = [\n",
        "                np.random.uniform(0.5, 0.75),    # Medium similarity\n",
        "                np.random.uniform(8, 48),        # Medium time\n",
        "                np.random.uniform(2, 8),         # Medium distance\n",
        "                np.random.uniform(0.4, 0.7),     # Medium user score\n",
        "                np.random.uniform(0.5, 0.8),\n",
        "                np.random.uniform(0.8, 1.2),\n",
        "                np.random.uniform(0.4, 0.8),\n",
        "                np.random.uniform(0.4, 0.8),\n",
        "                np.random.uniform(0.4, 0.8),\n",
        "                np.random.uniform(0.6, 0.9),\n",
        "                label\n",
        "            ]\n",
        "            data.append(features)\n",
        "\n",
        "        # Shuffle data\n",
        "        np.random.shuffle(data)\n",
        "\n",
        "        columns = ['similarity', 'time_diff', 'location_diff', 'user_score',\n",
        "                  'waste_type_match', 'quantity_ratio', 'lighting_score',\n",
        "                  'angle_consistency', 'background_match', 'timestamp_validity', 'is_fraud']\n",
        "\n",
        "        df = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "        print(f\"✅ Realistic dataset created: {len(df)} samples\")\n",
        "        print(f\"📊 Legitimate: {len(df[df['is_fraud']==0])}, Fraud: {len(df[df['is_fraud']==1])}\")\n",
        "        print(f\"📈 Class distribution: {df['is_fraud'].value_counts(normalize=True)}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "def create_regularized_model():\n",
        "    \"\"\"Create model with proper regularization\"\"\"\n",
        "    inputs = tf.keras.Input(shape=(10,), name='features')\n",
        "\n",
        "    # Add input dropout\n",
        "    x = tf.keras.layers.Dropout(0.1)(inputs)\n",
        "\n",
        "    # Smaller layers with more regularization\n",
        "    x = tf.keras.layers.Dense(64, activation='relu',\n",
        "                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.01, l2=0.01))(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(0.3)(x)\n",
        "\n",
        "    x = tf.keras.layers.Dense(32, activation='relu',\n",
        "                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.01, l2=0.01))(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(0.4)(x)\n",
        "\n",
        "    x = tf.keras.layers.Dense(16, activation='relu',\n",
        "                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.01, l2=0.01))(x)\n",
        "    x = tf.keras.layers.Dropout(0.3)(x)\n",
        "\n",
        "    # Output layer\n",
        "    outputs = tf.keras.layers.Dense(1, activation='sigmoid', name='fraud_prob')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs, outputs, name='RecycloFraudDetector_v2')\n",
        "\n",
        "    # Use lower learning rate\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', 'precision', 'recall']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def improved_training():\n",
        "    \"\"\"Improved training with realistic expectations\"\"\"\n",
        "    print(\"🚀 Starting IMPROVED Fraud Detection Training\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 1. Generate realistic data\n",
        "    data_generator = ImprovedDataGenerator()\n",
        "    df = data_generator.create_realistic_dataset(num_samples=10000)\n",
        "\n",
        "    # 2. Feature analysis\n",
        "    print(\"\\n📊 Feature Analysis:\")\n",
        "    print(df.describe())\n",
        "\n",
        "    # 3. Prepare data\n",
        "    feature_columns = ['similarity', 'time_diff', 'location_diff', 'user_score',\n",
        "                      'waste_type_match', 'quantity_ratio', 'lighting_score',\n",
        "                      'angle_consistency', 'background_match', 'timestamp_validity']\n",
        "\n",
        "    X = df[feature_columns].values\n",
        "    y = df['is_fraud'].values\n",
        "\n",
        "    # 4. Robust train/val/test split\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "        X, y, test_size=0.4, random_state=42, stratify=y\n",
        "    )\n",
        "    X_val, X_test, y_val, y_test = train_test_split(\n",
        "        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
        "    )\n",
        "\n",
        "    # 5. Scaling\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    print(f\"📊 Final data splits:\")\n",
        "    print(f\"Train: {len(X_train)} ({np.mean(y_train):.3f} fraud rate)\")\n",
        "    print(f\"Val: {len(X_val)} ({np.mean(y_val):.3f} fraud rate)\")\n",
        "    print(f\"Test: {len(X_test)} ({np.mean(y_test):.3f} fraud rate)\")\n",
        "\n",
        "    # 6. Create model with regularization\n",
        "    model = create_regularized_model()\n",
        "    model.summary()\n",
        "\n",
        "    # 7. Training with realistic expectations\n",
        "    class_weight = {0: 1.0, 1: 2.0}  # Moderate class weight\n",
        "\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=15,\n",
        "            restore_best_weights=True\n",
        "        ),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=8,\n",
        "            min_lr=1e-6\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train_scaled, y_train,\n",
        "        epochs=200,\n",
        "        batch_size=64,  # Larger batch size\n",
        "        validation_data=(X_val_scaled, y_val),\n",
        "        class_weight=class_weight,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # 8. Final evaluation\n",
        "    test_pred_prob = model.predict(X_test_scaled)\n",
        "    test_pred = (test_pred_prob > 0.5).astype(int)\n",
        "\n",
        "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "    results = {\n",
        "        'accuracy': accuracy_score(y_test, test_pred),\n",
        "        'precision': precision_score(y_test, test_pred),\n",
        "        'recall': recall_score(y_test, test_pred),\n",
        "        'f1_score': f1_score(y_test, test_pred),\n",
        "        'auc_score': roc_auc_score(y_test, test_pred_prob),\n",
        "    }\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"🎯 REALISTIC MODEL EVALUATION\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"✅ Accuracy:  {results['accuracy']:.4f} (Target: 0.85-0.95)\")\n",
        "    print(f\"🎯 Precision: {results['precision']:.4f} (Target: 0.80-0.90)\")\n",
        "    print(f\"📊 Recall:    {results['recall']:.4f} (Target: 0.75-0.90)\")\n",
        "    print(f\"📈 F1-Score:  {results['f1_score']:.4f} (Target: 0.80-0.90)\")\n",
        "    print(f\"📈 AUC Score: {results['auc_score']:.4f} (Target: 0.85-0.95)\")\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, test_pred)\n",
        "    print(f\"\\nConfusion Matrix:\")\n",
        "    print(f\"TN: {cm[0][0]}, FP: {cm[0][1]}\")\n",
        "    print(f\"FN: {cm[1][0]}, TP: {cm[1][1]}\")\n",
        "\n",
        "    # Business metrics\n",
        "    fpr = cm[0][1] / (cm[0][0] + cm[0][1])\n",
        "    fnr = cm[1][0] / (cm[1][0] + cm[1][1])\n",
        "\n",
        "    print(f\"\\n💼 Business Impact:\")\n",
        "    print(f\"False Positive Rate: {fpr:.4f} (Target: <0.10)\")\n",
        "    print(f\"False Negative Rate: {fnr:.4f} (Target: <0.15)\")\n",
        "\n",
        "    # Save model\n",
        "    model.save('recyclo_fraud_detector_v2.h5')\n",
        "    with open('feature_scaler_v2.pkl', 'wb') as f:\n",
        "        pickle.dump(scaler, f)\n",
        "\n",
        "    print(\"\\n✅ Improved model saved!\")\n",
        "\n",
        "    return model, scaler, results, history\n",
        "\n",
        "# Run improved training\n",
        "if __name__ == \"__main__\":\n",
        "    model, scaler, results, history = improved_training()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ec4i-kQXsZGP",
        "outputId": "fe063f2a-30a8-4542-fdc7-8422f07e2934"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting IMPROVED Fraud Detection Training\n",
            "============================================================\n",
            "🔄 Generating realistic training data with noise...\n",
            "✅ Realistic dataset created: 10000 samples\n",
            "📊 Legitimate: 7753, Fraud: 2247\n",
            "📈 Class distribution: is_fraud\n",
            "0    0.7753\n",
            "1    0.2247\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "📊 Feature Analysis:\n",
            "         similarity     time_diff  location_diff    user_score  \\\n",
            "count  10000.000000  10000.000000   10000.000000  10000.000000   \n",
            "mean       0.730198     19.161910       5.586155      0.575040   \n",
            "std        0.184078     47.059710      14.298398      0.229180   \n",
            "min        0.101898      0.058523       0.001723      0.000207   \n",
            "25%        0.649421      1.924437       0.392845      0.457821   \n",
            "50%        0.763760      3.961086       1.159025      0.617367   \n",
            "75%        0.861202     11.532417       3.444949      0.743135   \n",
            "max        1.000000    335.847625      99.743230      0.988831   \n",
            "\n",
            "       waste_type_match  quantity_ratio  lighting_score  angle_consistency  \\\n",
            "count      10000.000000    10000.000000    10000.000000       10000.000000   \n",
            "mean           0.769141        1.014748        0.568774           0.643512   \n",
            "std            0.205935        0.207514        0.150944           0.159482   \n",
            "min            0.200364        0.304337        0.119607           0.152387   \n",
            "25%            0.655989        0.889772        0.460903           0.538641   \n",
            "50%            0.800000        1.006546        0.571844           0.658460   \n",
            "75%            0.900000        1.123411        0.682131           0.763355   \n",
            "max            1.000000        1.998800        0.972805           0.986803   \n",
            "\n",
            "       background_match  timestamp_validity      is_fraud  \n",
            "count      10000.000000        10000.000000  10000.000000  \n",
            "mean           0.475645            0.808319      0.224700  \n",
            "std            0.154894            0.215586      0.417405  \n",
            "min            0.064489            0.100117      0.000000  \n",
            "25%            0.364018            0.724839      0.000000  \n",
            "50%            0.476210            0.900000      0.000000  \n",
            "75%            0.592106            1.000000      0.000000  \n",
            "max            0.929884            1.000000      1.000000  \n",
            "📊 Final data splits:\n",
            "Train: 6000 (0.225 fraud rate)\n",
            "Val: 2000 (0.225 fraud rate)\n",
            "Test: 2000 (0.225 fraud rate)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"RecycloFraudDetector_v2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"RecycloFraudDetector_v2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ features (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m704\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ fraud_prob (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ features (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">704</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ fraud_prob (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,713\u001b[0m (14.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,713</span> (14.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,521\u001b[0m (13.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,521</span> (13.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m192\u001b[0m (768.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> (768.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.6959 - loss: 5.8890 - precision: 0.3787 - recall: 0.5366 - val_accuracy: 0.9700 - val_loss: 4.9200 - val_precision: 1.0000 - val_recall: 0.8667 - learning_rate: 5.0000e-04\n",
            "Epoch 2/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8975 - loss: 4.7436 - precision: 0.7446 - recall: 0.8422 - val_accuracy: 0.9695 - val_loss: 4.0105 - val_precision: 0.9974 - val_recall: 0.8667 - learning_rate: 5.0000e-04\n",
            "Epoch 3/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9376 - loss: 3.8770 - precision: 0.8636 - recall: 0.8597 - val_accuracy: 0.9700 - val_loss: 3.1961 - val_precision: 0.9974 - val_recall: 0.8689 - learning_rate: 5.0000e-04\n",
            "Epoch 4/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9557 - loss: 3.0844 - precision: 0.9141 - recall: 0.8797 - val_accuracy: 0.9695 - val_loss: 2.4854 - val_precision: 0.9949 - val_recall: 0.8689 - learning_rate: 5.0000e-04\n",
            "Epoch 5/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9655 - loss: 2.4032 - precision: 0.9469 - recall: 0.8974 - val_accuracy: 0.9690 - val_loss: 1.8850 - val_precision: 0.9924 - val_recall: 0.8689 - learning_rate: 5.0000e-04\n",
            "Epoch 6/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9660 - loss: 1.8124 - precision: 0.9429 - recall: 0.9027 - val_accuracy: 0.9695 - val_loss: 1.3942 - val_precision: 0.9924 - val_recall: 0.8711 - learning_rate: 5.0000e-04\n",
            "Epoch 7/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9696 - loss: 1.3363 - precision: 0.9504 - recall: 0.9151 - val_accuracy: 0.9690 - val_loss: 1.0208 - val_precision: 0.9949 - val_recall: 0.8667 - learning_rate: 5.0000e-04\n",
            "Epoch 8/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9641 - loss: 0.9896 - precision: 0.9370 - recall: 0.9105 - val_accuracy: 0.9650 - val_loss: 0.7675 - val_precision: 1.0000 - val_recall: 0.8444 - learning_rate: 5.0000e-04\n",
            "Epoch 9/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9664 - loss: 0.7230 - precision: 0.9169 - recall: 0.9308 - val_accuracy: 0.9655 - val_loss: 0.5865 - val_precision: 1.0000 - val_recall: 0.8467 - learning_rate: 5.0000e-04\n",
            "Epoch 10/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9693 - loss: 0.5507 - precision: 0.9264 - recall: 0.9385 - val_accuracy: 0.9695 - val_loss: 0.4511 - val_precision: 0.9974 - val_recall: 0.8667 - learning_rate: 5.0000e-04\n",
            "Epoch 11/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9700 - loss: 0.4393 - precision: 0.9430 - recall: 0.9184 - val_accuracy: 0.9695 - val_loss: 0.3597 - val_precision: 0.9974 - val_recall: 0.8667 - learning_rate: 5.0000e-04\n",
            "Epoch 12/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9651 - loss: 0.3764 - precision: 0.9232 - recall: 0.9220 - val_accuracy: 0.9695 - val_loss: 0.3030 - val_precision: 0.9949 - val_recall: 0.8689 - learning_rate: 5.0000e-04\n",
            "Epoch 13/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9635 - loss: 0.3436 - precision: 0.9226 - recall: 0.9122 - val_accuracy: 0.9685 - val_loss: 0.2574 - val_precision: 0.9663 - val_recall: 0.8911 - learning_rate: 5.0000e-04\n",
            "Epoch 14/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9644 - loss: 0.3112 - precision: 0.9153 - recall: 0.9268 - val_accuracy: 0.9680 - val_loss: 0.2308 - val_precision: 0.9573 - val_recall: 0.8978 - learning_rate: 5.0000e-04\n",
            "Epoch 15/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9728 - loss: 0.2827 - precision: 0.9404 - recall: 0.9372 - val_accuracy: 0.9685 - val_loss: 0.2149 - val_precision: 0.9574 - val_recall: 0.9000 - learning_rate: 5.0000e-04\n",
            "Epoch 16/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9628 - loss: 0.2882 - precision: 0.9181 - recall: 0.9198 - val_accuracy: 0.9685 - val_loss: 0.2120 - val_precision: 0.9108 - val_recall: 0.9533 - learning_rate: 5.0000e-04\n",
            "Epoch 17/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9701 - loss: 0.2619 - precision: 0.9295 - recall: 0.9427 - val_accuracy: 0.9665 - val_loss: 0.2008 - val_precision: 0.9592 - val_recall: 0.8889 - learning_rate: 5.0000e-04\n",
            "Epoch 18/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9611 - loss: 0.2759 - precision: 0.9193 - recall: 0.9082 - val_accuracy: 0.9675 - val_loss: 0.1963 - val_precision: 0.9104 - val_recall: 0.9489 - learning_rate: 5.0000e-04\n",
            "Epoch 19/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9645 - loss: 0.2577 - precision: 0.9189 - recall: 0.9253 - val_accuracy: 0.9685 - val_loss: 0.1834 - val_precision: 0.9663 - val_recall: 0.8911 - learning_rate: 5.0000e-04\n",
            "Epoch 20/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9661 - loss: 0.2490 - precision: 0.9272 - recall: 0.9210 - val_accuracy: 0.9675 - val_loss: 0.1822 - val_precision: 0.9212 - val_recall: 0.9356 - learning_rate: 5.0000e-04\n",
            "Epoch 21/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9683 - loss: 0.2428 - precision: 0.9258 - recall: 0.9367 - val_accuracy: 0.9680 - val_loss: 0.1750 - val_precision: 0.9232 - val_recall: 0.9356 - learning_rate: 5.0000e-04\n",
            "Epoch 22/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9643 - loss: 0.2483 - precision: 0.9241 - recall: 0.9229 - val_accuracy: 0.9675 - val_loss: 0.1724 - val_precision: 0.9639 - val_recall: 0.8889 - learning_rate: 5.0000e-04\n",
            "Epoch 23/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9719 - loss: 0.2301 - precision: 0.9511 - recall: 0.9250 - val_accuracy: 0.9690 - val_loss: 0.1764 - val_precision: 0.9042 - val_recall: 0.9644 - learning_rate: 5.0000e-04\n",
            "Epoch 24/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9597 - loss: 0.2477 - precision: 0.9016 - recall: 0.9205 - val_accuracy: 0.9665 - val_loss: 0.1667 - val_precision: 0.9284 - val_recall: 0.9222 - learning_rate: 5.0000e-04\n",
            "Epoch 25/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9648 - loss: 0.2390 - precision: 0.9271 - recall: 0.9198 - val_accuracy: 0.9655 - val_loss: 0.1736 - val_precision: 0.8977 - val_recall: 0.9556 - learning_rate: 5.0000e-04\n",
            "Epoch 26/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9676 - loss: 0.2234 - precision: 0.9228 - recall: 0.9356 - val_accuracy: 0.9670 - val_loss: 0.1641 - val_precision: 0.9103 - val_recall: 0.9467 - learning_rate: 5.0000e-04\n",
            "Epoch 27/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9623 - loss: 0.2385 - precision: 0.9080 - recall: 0.9258 - val_accuracy: 0.9675 - val_loss: 0.1579 - val_precision: 0.9287 - val_recall: 0.9267 - learning_rate: 5.0000e-04\n",
            "Epoch 28/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9650 - loss: 0.2231 - precision: 0.9245 - recall: 0.9177 - val_accuracy: 0.9670 - val_loss: 0.1571 - val_precision: 0.9229 - val_recall: 0.9311 - learning_rate: 5.0000e-04\n",
            "Epoch 29/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9598 - loss: 0.2346 - precision: 0.9136 - recall: 0.9108 - val_accuracy: 0.9680 - val_loss: 0.1533 - val_precision: 0.9270 - val_recall: 0.9311 - learning_rate: 5.0000e-04\n",
            "Epoch 30/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9619 - loss: 0.2364 - precision: 0.9103 - recall: 0.9210 - val_accuracy: 0.9670 - val_loss: 0.1548 - val_precision: 0.9324 - val_recall: 0.9200 - learning_rate: 5.0000e-04\n",
            "Epoch 31/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9685 - loss: 0.2233 - precision: 0.9380 - recall: 0.9253 - val_accuracy: 0.9655 - val_loss: 0.1556 - val_precision: 0.9243 - val_recall: 0.9222 - learning_rate: 5.0000e-04\n",
            "Epoch 32/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9687 - loss: 0.2152 - precision: 0.9337 - recall: 0.9337 - val_accuracy: 0.9665 - val_loss: 0.1531 - val_precision: 0.9323 - val_recall: 0.9178 - learning_rate: 5.0000e-04\n",
            "Epoch 33/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9622 - loss: 0.2277 - precision: 0.9205 - recall: 0.9111 - val_accuracy: 0.9675 - val_loss: 0.1521 - val_precision: 0.9616 - val_recall: 0.8911 - learning_rate: 5.0000e-04\n",
            "Epoch 34/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9674 - loss: 0.2137 - precision: 0.9276 - recall: 0.9253 - val_accuracy: 0.9665 - val_loss: 0.1516 - val_precision: 0.9227 - val_recall: 0.9289 - learning_rate: 5.0000e-04\n",
            "Epoch 35/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9682 - loss: 0.2064 - precision: 0.9237 - recall: 0.9360 - val_accuracy: 0.9665 - val_loss: 0.1502 - val_precision: 0.9154 - val_recall: 0.9378 - learning_rate: 5.0000e-04\n",
            "Epoch 36/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9670 - loss: 0.2167 - precision: 0.9288 - recall: 0.9233 - val_accuracy: 0.9695 - val_loss: 0.1470 - val_precision: 0.9294 - val_recall: 0.9356 - learning_rate: 5.0000e-04\n",
            "Epoch 37/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9706 - loss: 0.2029 - precision: 0.9321 - recall: 0.9363 - val_accuracy: 0.9655 - val_loss: 0.1482 - val_precision: 0.9205 - val_recall: 0.9267 - learning_rate: 5.0000e-04\n",
            "Epoch 38/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9686 - loss: 0.2092 - precision: 0.9343 - recall: 0.9249 - val_accuracy: 0.9665 - val_loss: 0.1506 - val_precision: 0.9190 - val_recall: 0.9333 - learning_rate: 5.0000e-04\n",
            "Epoch 39/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9665 - loss: 0.2018 - precision: 0.9245 - recall: 0.9296 - val_accuracy: 0.9685 - val_loss: 0.1511 - val_precision: 0.9091 - val_recall: 0.9556 - learning_rate: 5.0000e-04\n",
            "Epoch 40/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9644 - loss: 0.2104 - precision: 0.9139 - recall: 0.9230 - val_accuracy: 0.9685 - val_loss: 0.1463 - val_precision: 0.9179 - val_recall: 0.9444 - learning_rate: 5.0000e-04\n",
            "Epoch 41/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9697 - loss: 0.2079 - precision: 0.9305 - recall: 0.9387 - val_accuracy: 0.9675 - val_loss: 0.1428 - val_precision: 0.9639 - val_recall: 0.8889 - learning_rate: 5.0000e-04\n",
            "Epoch 42/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9722 - loss: 0.1948 - precision: 0.9480 - recall: 0.9224 - val_accuracy: 0.9675 - val_loss: 0.1431 - val_precision: 0.9122 - val_recall: 0.9467 - learning_rate: 5.0000e-04\n",
            "Epoch 43/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9676 - loss: 0.1950 - precision: 0.9241 - recall: 0.9354 - val_accuracy: 0.9655 - val_loss: 0.1389 - val_precision: 0.9568 - val_recall: 0.8867 - learning_rate: 5.0000e-04\n",
            "Epoch 44/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9662 - loss: 0.2100 - precision: 0.9279 - recall: 0.9246 - val_accuracy: 0.9705 - val_loss: 0.1399 - val_precision: 0.9316 - val_recall: 0.9378 - learning_rate: 5.0000e-04\n",
            "Epoch 45/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9661 - loss: 0.2007 - precision: 0.9192 - recall: 0.9303 - val_accuracy: 0.9670 - val_loss: 0.1513 - val_precision: 0.8934 - val_recall: 0.9689 - learning_rate: 5.0000e-04\n",
            "Epoch 46/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9616 - loss: 0.2098 - precision: 0.9099 - recall: 0.9188 - val_accuracy: 0.9690 - val_loss: 0.1397 - val_precision: 0.9181 - val_recall: 0.9467 - learning_rate: 5.0000e-04\n",
            "Epoch 47/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9679 - loss: 0.1936 - precision: 0.9242 - recall: 0.9347 - val_accuracy: 0.9665 - val_loss: 0.1440 - val_precision: 0.9083 - val_recall: 0.9467 - learning_rate: 5.0000e-04\n",
            "Epoch 48/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9650 - loss: 0.1989 - precision: 0.9132 - recall: 0.9310 - val_accuracy: 0.9665 - val_loss: 0.1370 - val_precision: 0.9323 - val_recall: 0.9178 - learning_rate: 5.0000e-04\n",
            "Epoch 49/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9644 - loss: 0.1962 - precision: 0.9146 - recall: 0.9261 - val_accuracy: 0.9685 - val_loss: 0.1306 - val_precision: 0.9388 - val_recall: 0.9200 - learning_rate: 5.0000e-04\n",
            "Epoch 50/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9623 - loss: 0.2076 - precision: 0.9144 - recall: 0.9162 - val_accuracy: 0.9665 - val_loss: 0.1448 - val_precision: 0.9066 - val_recall: 0.9489 - learning_rate: 5.0000e-04\n",
            "Epoch 51/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9672 - loss: 0.1996 - precision: 0.9241 - recall: 0.9313 - val_accuracy: 0.9700 - val_loss: 0.1392 - val_precision: 0.9062 - val_recall: 0.9667 - learning_rate: 5.0000e-04\n",
            "Epoch 52/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9661 - loss: 0.1992 - precision: 0.9079 - recall: 0.9414 - val_accuracy: 0.9675 - val_loss: 0.1343 - val_precision: 0.9231 - val_recall: 0.9333 - learning_rate: 5.0000e-04\n",
            "Epoch 53/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9738 - loss: 0.1760 - precision: 0.9465 - recall: 0.9390 - val_accuracy: 0.9675 - val_loss: 0.1387 - val_precision: 0.9122 - val_recall: 0.9467 - learning_rate: 5.0000e-04\n",
            "Epoch 54/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9686 - loss: 0.1903 - precision: 0.9283 - recall: 0.9334 - val_accuracy: 0.9690 - val_loss: 0.1317 - val_precision: 0.9389 - val_recall: 0.9222 - learning_rate: 5.0000e-04\n",
            "Epoch 55/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9673 - loss: 0.1896 - precision: 0.9179 - recall: 0.9330 - val_accuracy: 0.9675 - val_loss: 0.1359 - val_precision: 0.9140 - val_recall: 0.9444 - learning_rate: 5.0000e-04\n",
            "Epoch 56/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9718 - loss: 0.1801 - precision: 0.9323 - recall: 0.9415 - val_accuracy: 0.9660 - val_loss: 0.1356 - val_precision: 0.9591 - val_recall: 0.8867 - learning_rate: 5.0000e-04\n",
            "Epoch 57/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9728 - loss: 0.1843 - precision: 0.9360 - recall: 0.9345 - val_accuracy: 0.9650 - val_loss: 0.1371 - val_precision: 0.9043 - val_recall: 0.9444 - learning_rate: 5.0000e-04\n",
            "Epoch 58/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9673 - loss: 0.1848 - precision: 0.9177 - recall: 0.9411 - val_accuracy: 0.9675 - val_loss: 0.1304 - val_precision: 0.9176 - val_recall: 0.9400 - learning_rate: 2.5000e-04\n",
            "Epoch 59/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9694 - loss: 0.1820 - precision: 0.9253 - recall: 0.9379 - val_accuracy: 0.9680 - val_loss: 0.1291 - val_precision: 0.9177 - val_recall: 0.9422 - learning_rate: 2.5000e-04\n",
            "Epoch 60/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9710 - loss: 0.1799 - precision: 0.9332 - recall: 0.9372 - val_accuracy: 0.9680 - val_loss: 0.1302 - val_precision: 0.9124 - val_recall: 0.9489 - learning_rate: 2.5000e-04\n",
            "Epoch 61/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9682 - loss: 0.1993 - precision: 0.9367 - recall: 0.9206 - val_accuracy: 0.9680 - val_loss: 0.1274 - val_precision: 0.9159 - val_recall: 0.9444 - learning_rate: 2.5000e-04\n",
            "Epoch 62/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9663 - loss: 0.1890 - precision: 0.9165 - recall: 0.9295 - val_accuracy: 0.9700 - val_loss: 0.1254 - val_precision: 0.9314 - val_recall: 0.9356 - learning_rate: 2.5000e-04\n",
            "Epoch 63/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9690 - loss: 0.1847 - precision: 0.9288 - recall: 0.9382 - val_accuracy: 0.9675 - val_loss: 0.1281 - val_precision: 0.9087 - val_recall: 0.9511 - learning_rate: 2.5000e-04\n",
            "Epoch 64/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9731 - loss: 0.1676 - precision: 0.9264 - recall: 0.9505 - val_accuracy: 0.9700 - val_loss: 0.1313 - val_precision: 0.9062 - val_recall: 0.9667 - learning_rate: 2.5000e-04\n",
            "Epoch 65/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9674 - loss: 0.1837 - precision: 0.9154 - recall: 0.9436 - val_accuracy: 0.9690 - val_loss: 0.1267 - val_precision: 0.9145 - val_recall: 0.9511 - learning_rate: 2.5000e-04\n",
            "Epoch 66/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9680 - loss: 0.1784 - precision: 0.9123 - recall: 0.9475 - val_accuracy: 0.9675 - val_loss: 0.1264 - val_precision: 0.9140 - val_recall: 0.9444 - learning_rate: 2.5000e-04\n",
            "Epoch 67/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9636 - loss: 0.1875 - precision: 0.9067 - recall: 0.9290 - val_accuracy: 0.9675 - val_loss: 0.1298 - val_precision: 0.9036 - val_recall: 0.9578 - learning_rate: 2.5000e-04\n",
            "Epoch 68/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9675 - loss: 0.1762 - precision: 0.9309 - recall: 0.9265 - val_accuracy: 0.9665 - val_loss: 0.1242 - val_precision: 0.9172 - val_recall: 0.9356 - learning_rate: 2.5000e-04\n",
            "Epoch 69/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9702 - loss: 0.1808 - precision: 0.9249 - recall: 0.9441 - val_accuracy: 0.9690 - val_loss: 0.1279 - val_precision: 0.9093 - val_recall: 0.9578 - learning_rate: 2.5000e-04\n",
            "Epoch 70/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9685 - loss: 0.1833 - precision: 0.9261 - recall: 0.9348 - val_accuracy: 0.9675 - val_loss: 0.1250 - val_precision: 0.9122 - val_recall: 0.9467 - learning_rate: 2.5000e-04\n",
            "Epoch 71/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9694 - loss: 0.1791 - precision: 0.9294 - recall: 0.9370 - val_accuracy: 0.9655 - val_loss: 0.1215 - val_precision: 0.9359 - val_recall: 0.9089 - learning_rate: 2.5000e-04\n",
            "Epoch 72/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9652 - loss: 0.1966 - precision: 0.9330 - recall: 0.9177 - val_accuracy: 0.9690 - val_loss: 0.1212 - val_precision: 0.9330 - val_recall: 0.9289 - learning_rate: 2.5000e-04\n",
            "Epoch 73/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9702 - loss: 0.1707 - precision: 0.9314 - recall: 0.9366 - val_accuracy: 0.9685 - val_loss: 0.1266 - val_precision: 0.9057 - val_recall: 0.9600 - learning_rate: 2.5000e-04\n",
            "Epoch 74/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9681 - loss: 0.1742 - precision: 0.9222 - recall: 0.9389 - val_accuracy: 0.9695 - val_loss: 0.1284 - val_precision: 0.9044 - val_recall: 0.9667 - learning_rate: 2.5000e-04\n",
            "Epoch 75/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9708 - loss: 0.1721 - precision: 0.9244 - recall: 0.9439 - val_accuracy: 0.9675 - val_loss: 0.1254 - val_precision: 0.9053 - val_recall: 0.9556 - learning_rate: 2.5000e-04\n",
            "Epoch 76/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9699 - loss: 0.1794 - precision: 0.9249 - recall: 0.9422 - val_accuracy: 0.9680 - val_loss: 0.1220 - val_precision: 0.9124 - val_recall: 0.9489 - learning_rate: 2.5000e-04\n",
            "Epoch 77/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9649 - loss: 0.1915 - precision: 0.9169 - recall: 0.9274 - val_accuracy: 0.9680 - val_loss: 0.1233 - val_precision: 0.9106 - val_recall: 0.9511 - learning_rate: 2.5000e-04\n",
            "Epoch 78/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9654 - loss: 0.1860 - precision: 0.9125 - recall: 0.9357 - val_accuracy: 0.9680 - val_loss: 0.1251 - val_precision: 0.9124 - val_recall: 0.9489 - learning_rate: 2.5000e-04\n",
            "Epoch 79/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9685 - loss: 0.1746 - precision: 0.9204 - recall: 0.9438 - val_accuracy: 0.9675 - val_loss: 0.1221 - val_precision: 0.9194 - val_recall: 0.9378 - learning_rate: 2.5000e-04\n",
            "Epoch 80/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9658 - loss: 0.1765 - precision: 0.9151 - recall: 0.9341 - val_accuracy: 0.9695 - val_loss: 0.1203 - val_precision: 0.9294 - val_recall: 0.9356 - learning_rate: 2.5000e-04\n",
            "Epoch 81/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9620 - loss: 0.1836 - precision: 0.9074 - recall: 0.9269 - val_accuracy: 0.9660 - val_loss: 0.1259 - val_precision: 0.9013 - val_recall: 0.9533 - learning_rate: 2.5000e-04\n",
            "Epoch 82/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9623 - loss: 0.1876 - precision: 0.9014 - recall: 0.9290 - val_accuracy: 0.9670 - val_loss: 0.1232 - val_precision: 0.9138 - val_recall: 0.9422 - learning_rate: 2.5000e-04\n",
            "Epoch 83/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9640 - loss: 0.1812 - precision: 0.9143 - recall: 0.9265 - val_accuracy: 0.9670 - val_loss: 0.1187 - val_precision: 0.9324 - val_recall: 0.9200 - learning_rate: 2.5000e-04\n",
            "Epoch 84/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9682 - loss: 0.1736 - precision: 0.9275 - recall: 0.9289 - val_accuracy: 0.9675 - val_loss: 0.1212 - val_precision: 0.9087 - val_recall: 0.9511 - learning_rate: 2.5000e-04\n",
            "Epoch 85/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9594 - loss: 0.1935 - precision: 0.8952 - recall: 0.9298 - val_accuracy: 0.9680 - val_loss: 0.1195 - val_precision: 0.9159 - val_recall: 0.9444 - learning_rate: 2.5000e-04\n",
            "Epoch 86/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9647 - loss: 0.1853 - precision: 0.9209 - recall: 0.9254 - val_accuracy: 0.9675 - val_loss: 0.1233 - val_precision: 0.9053 - val_recall: 0.9556 - learning_rate: 2.5000e-04\n",
            "Epoch 87/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9641 - loss: 0.1791 - precision: 0.8998 - recall: 0.9434 - val_accuracy: 0.9680 - val_loss: 0.1207 - val_precision: 0.9124 - val_recall: 0.9489 - learning_rate: 2.5000e-04\n",
            "Epoch 88/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9646 - loss: 0.1997 - precision: 0.9173 - recall: 0.9258 - val_accuracy: 0.9685 - val_loss: 0.1182 - val_precision: 0.9234 - val_recall: 0.9378 - learning_rate: 2.5000e-04\n",
            "Epoch 89/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9621 - loss: 0.1879 - precision: 0.9148 - recall: 0.9151 - val_accuracy: 0.9675 - val_loss: 0.1201 - val_precision: 0.9140 - val_recall: 0.9444 - learning_rate: 2.5000e-04\n",
            "Epoch 90/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9761 - loss: 0.1671 - precision: 0.9514 - recall: 0.9430 - val_accuracy: 0.9675 - val_loss: 0.1217 - val_precision: 0.9087 - val_recall: 0.9511 - learning_rate: 2.5000e-04\n",
            "Epoch 91/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9663 - loss: 0.1790 - precision: 0.9203 - recall: 0.9341 - val_accuracy: 0.9685 - val_loss: 0.1169 - val_precision: 0.9216 - val_recall: 0.9400 - learning_rate: 2.5000e-04\n",
            "Epoch 92/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9691 - loss: 0.1777 - precision: 0.9250 - recall: 0.9351 - val_accuracy: 0.9675 - val_loss: 0.1277 - val_precision: 0.8969 - val_recall: 0.9667 - learning_rate: 2.5000e-04\n",
            "Epoch 93/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9697 - loss: 0.1663 - precision: 0.9163 - recall: 0.9505 - val_accuracy: 0.9685 - val_loss: 0.1210 - val_precision: 0.9074 - val_recall: 0.9578 - learning_rate: 2.5000e-04\n",
            "Epoch 94/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9682 - loss: 0.1776 - precision: 0.9256 - recall: 0.9342 - val_accuracy: 0.9685 - val_loss: 0.1227 - val_precision: 0.9074 - val_recall: 0.9578 - learning_rate: 2.5000e-04\n",
            "Epoch 95/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9599 - loss: 0.1857 - precision: 0.8998 - recall: 0.9267 - val_accuracy: 0.9665 - val_loss: 0.1199 - val_precision: 0.9118 - val_recall: 0.9422 - learning_rate: 2.5000e-04\n",
            "Epoch 96/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9703 - loss: 0.1736 - precision: 0.9293 - recall: 0.9417 - val_accuracy: 0.9660 - val_loss: 0.1186 - val_precision: 0.9341 - val_recall: 0.9133 - learning_rate: 2.5000e-04\n",
            "Epoch 97/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9706 - loss: 0.1677 - precision: 0.9237 - recall: 0.9419 - val_accuracy: 0.9685 - val_loss: 0.1195 - val_precision: 0.9126 - val_recall: 0.9511 - learning_rate: 2.5000e-04\n",
            "Epoch 98/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9715 - loss: 0.1661 - precision: 0.9291 - recall: 0.9472 - val_accuracy: 0.9680 - val_loss: 0.1162 - val_precision: 0.9406 - val_recall: 0.9156 - learning_rate: 2.5000e-04\n",
            "Epoch 99/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9679 - loss: 0.1826 - precision: 0.9324 - recall: 0.9284 - val_accuracy: 0.9695 - val_loss: 0.1158 - val_precision: 0.9237 - val_recall: 0.9422 - learning_rate: 2.5000e-04\n",
            "Epoch 100/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9698 - loss: 0.1731 - precision: 0.9276 - recall: 0.9315 - val_accuracy: 0.9690 - val_loss: 0.1160 - val_precision: 0.9163 - val_recall: 0.9489 - learning_rate: 2.5000e-04\n",
            "Epoch 101/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9658 - loss: 0.1792 - precision: 0.9173 - recall: 0.9310 - val_accuracy: 0.9670 - val_loss: 0.1189 - val_precision: 0.9034 - val_recall: 0.9556 - learning_rate: 2.5000e-04\n",
            "Epoch 102/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9660 - loss: 0.1695 - precision: 0.9139 - recall: 0.9383 - val_accuracy: 0.9675 - val_loss: 0.1223 - val_precision: 0.9002 - val_recall: 0.9622 - learning_rate: 2.5000e-04\n",
            "Epoch 103/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9607 - loss: 0.1825 - precision: 0.9015 - recall: 0.9265 - val_accuracy: 0.9695 - val_loss: 0.1224 - val_precision: 0.9044 - val_recall: 0.9667 - learning_rate: 2.5000e-04\n",
            "Epoch 104/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9701 - loss: 0.1615 - precision: 0.9258 - recall: 0.9444 - val_accuracy: 0.9670 - val_loss: 0.1191 - val_precision: 0.9103 - val_recall: 0.9467 - learning_rate: 2.5000e-04\n",
            "Epoch 105/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9678 - loss: 0.1725 - precision: 0.9165 - recall: 0.9420 - val_accuracy: 0.9670 - val_loss: 0.1171 - val_precision: 0.9034 - val_recall: 0.9556 - learning_rate: 2.5000e-04\n",
            "Epoch 106/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9627 - loss: 0.1742 - precision: 0.9015 - recall: 0.9343 - val_accuracy: 0.9685 - val_loss: 0.1184 - val_precision: 0.9126 - val_recall: 0.9511 - learning_rate: 2.5000e-04\n",
            "Epoch 107/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9746 - loss: 0.1608 - precision: 0.9354 - recall: 0.9550 - val_accuracy: 0.9675 - val_loss: 0.1193 - val_precision: 0.9104 - val_recall: 0.9489 - learning_rate: 2.5000e-04\n",
            "Epoch 108/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9673 - loss: 0.1745 - precision: 0.9188 - recall: 0.9376 - val_accuracy: 0.9675 - val_loss: 0.1187 - val_precision: 0.9087 - val_recall: 0.9511 - learning_rate: 1.2500e-04\n",
            "Epoch 109/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9657 - loss: 0.1701 - precision: 0.9091 - recall: 0.9460 - val_accuracy: 0.9680 - val_loss: 0.1159 - val_precision: 0.9106 - val_recall: 0.9511 - learning_rate: 1.2500e-04\n",
            "Epoch 110/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9667 - loss: 0.1731 - precision: 0.9145 - recall: 0.9422 - val_accuracy: 0.9680 - val_loss: 0.1151 - val_precision: 0.9177 - val_recall: 0.9422 - learning_rate: 1.2500e-04\n",
            "Epoch 111/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9670 - loss: 0.1692 - precision: 0.9238 - recall: 0.9267 - val_accuracy: 0.9670 - val_loss: 0.1178 - val_precision: 0.9034 - val_recall: 0.9556 - learning_rate: 1.2500e-04\n",
            "Epoch 112/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9659 - loss: 0.1683 - precision: 0.9032 - recall: 0.9465 - val_accuracy: 0.9675 - val_loss: 0.1177 - val_precision: 0.9087 - val_recall: 0.9511 - learning_rate: 1.2500e-04\n",
            "Epoch 113/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9623 - loss: 0.1645 - precision: 0.8969 - recall: 0.9378 - val_accuracy: 0.9690 - val_loss: 0.1151 - val_precision: 0.9128 - val_recall: 0.9533 - learning_rate: 1.2500e-04\n",
            "Epoch 114/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9653 - loss: 0.1723 - precision: 0.9138 - recall: 0.9303 - val_accuracy: 0.9685 - val_loss: 0.1150 - val_precision: 0.9091 - val_recall: 0.9556 - learning_rate: 1.2500e-04\n",
            "Epoch 115/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9674 - loss: 0.1664 - precision: 0.9181 - recall: 0.9340 - val_accuracy: 0.9680 - val_loss: 0.1156 - val_precision: 0.9089 - val_recall: 0.9533 - learning_rate: 1.2500e-04\n",
            "Epoch 116/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9673 - loss: 0.1650 - precision: 0.9260 - recall: 0.9301 - val_accuracy: 0.9680 - val_loss: 0.1156 - val_precision: 0.9106 - val_recall: 0.9511 - learning_rate: 1.2500e-04\n",
            "Epoch 117/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9678 - loss: 0.1682 - precision: 0.9161 - recall: 0.9437 - val_accuracy: 0.9680 - val_loss: 0.1153 - val_precision: 0.9055 - val_recall: 0.9578 - learning_rate: 1.2500e-04\n",
            "Epoch 118/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9668 - loss: 0.1686 - precision: 0.9101 - recall: 0.9412 - val_accuracy: 0.9690 - val_loss: 0.1166 - val_precision: 0.9059 - val_recall: 0.9622 - learning_rate: 1.2500e-04\n",
            "Epoch 119/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9705 - loss: 0.1585 - precision: 0.9192 - recall: 0.9496 - val_accuracy: 0.9690 - val_loss: 0.1198 - val_precision: 0.9008 - val_recall: 0.9689 - learning_rate: 1.2500e-04\n",
            "Epoch 120/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9647 - loss: 0.1626 - precision: 0.8958 - recall: 0.9504 - val_accuracy: 0.9675 - val_loss: 0.1175 - val_precision: 0.9053 - val_recall: 0.9556 - learning_rate: 1.2500e-04\n",
            "Epoch 121/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9675 - loss: 0.1638 - precision: 0.9195 - recall: 0.9433 - val_accuracy: 0.9670 - val_loss: 0.1132 - val_precision: 0.9138 - val_recall: 0.9422 - learning_rate: 1.2500e-04\n",
            "Epoch 122/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9681 - loss: 0.1683 - precision: 0.9161 - recall: 0.9416 - val_accuracy: 0.9675 - val_loss: 0.1166 - val_precision: 0.9070 - val_recall: 0.9533 - learning_rate: 1.2500e-04\n",
            "Epoch 123/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9649 - loss: 0.1768 - precision: 0.9152 - recall: 0.9330 - val_accuracy: 0.9665 - val_loss: 0.1152 - val_precision: 0.9066 - val_recall: 0.9489 - learning_rate: 1.2500e-04\n",
            "Epoch 124/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9592 - loss: 0.1747 - precision: 0.8817 - recall: 0.9332 - val_accuracy: 0.9665 - val_loss: 0.1147 - val_precision: 0.9101 - val_recall: 0.9444 - learning_rate: 1.2500e-04\n",
            "Epoch 125/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9722 - loss: 0.1627 - precision: 0.9318 - recall: 0.9455 - val_accuracy: 0.9675 - val_loss: 0.1154 - val_precision: 0.9087 - val_recall: 0.9511 - learning_rate: 1.2500e-04\n",
            "Epoch 126/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9676 - loss: 0.1720 - precision: 0.9159 - recall: 0.9410 - val_accuracy: 0.9695 - val_loss: 0.1175 - val_precision: 0.9044 - val_recall: 0.9667 - learning_rate: 1.2500e-04\n",
            "Epoch 127/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9646 - loss: 0.1633 - precision: 0.9055 - recall: 0.9397 - val_accuracy: 0.9680 - val_loss: 0.1157 - val_precision: 0.9089 - val_recall: 0.9533 - learning_rate: 1.2500e-04\n",
            "Epoch 128/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9736 - loss: 0.1525 - precision: 0.9326 - recall: 0.9507 - val_accuracy: 0.9690 - val_loss: 0.1154 - val_precision: 0.9076 - val_recall: 0.9600 - learning_rate: 1.2500e-04\n",
            "Epoch 129/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9640 - loss: 0.1674 - precision: 0.8967 - recall: 0.9424 - val_accuracy: 0.9695 - val_loss: 0.1187 - val_precision: 0.9044 - val_recall: 0.9667 - learning_rate: 1.2500e-04\n",
            "Epoch 130/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9662 - loss: 0.1688 - precision: 0.9110 - recall: 0.9422 - val_accuracy: 0.9670 - val_loss: 0.1131 - val_precision: 0.9138 - val_recall: 0.9422 - learning_rate: 6.2500e-05\n",
            "Epoch 131/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9685 - loss: 0.1631 - precision: 0.9207 - recall: 0.9390 - val_accuracy: 0.9665 - val_loss: 0.1163 - val_precision: 0.9015 - val_recall: 0.9556 - learning_rate: 6.2500e-05\n",
            "Epoch 132/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9651 - loss: 0.1772 - precision: 0.9047 - recall: 0.9435 - val_accuracy: 0.9670 - val_loss: 0.1140 - val_precision: 0.9156 - val_recall: 0.9400 - learning_rate: 6.2500e-05\n",
            "Epoch 133/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9711 - loss: 0.1645 - precision: 0.9375 - recall: 0.9359 - val_accuracy: 0.9675 - val_loss: 0.1134 - val_precision: 0.9231 - val_recall: 0.9333 - learning_rate: 6.2500e-05\n",
            "Epoch 134/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9694 - loss: 0.1574 - precision: 0.9358 - recall: 0.9292 - val_accuracy: 0.9680 - val_loss: 0.1143 - val_precision: 0.9106 - val_recall: 0.9511 - learning_rate: 6.2500e-05\n",
            "Epoch 135/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9647 - loss: 0.1662 - precision: 0.8979 - recall: 0.9415 - val_accuracy: 0.9685 - val_loss: 0.1149 - val_precision: 0.9091 - val_recall: 0.9556 - learning_rate: 6.2500e-05\n",
            "Epoch 136/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9717 - loss: 0.1549 - precision: 0.9219 - recall: 0.9493 - val_accuracy: 0.9680 - val_loss: 0.1131 - val_precision: 0.9089 - val_recall: 0.9533 - learning_rate: 6.2500e-05\n",
            "Epoch 137/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9622 - loss: 0.1676 - precision: 0.8978 - recall: 0.9349 - val_accuracy: 0.9675 - val_loss: 0.1153 - val_precision: 0.9036 - val_recall: 0.9578 - learning_rate: 6.2500e-05\n",
            "Epoch 138/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9687 - loss: 0.1598 - precision: 0.9189 - recall: 0.9433 - val_accuracy: 0.9675 - val_loss: 0.1156 - val_precision: 0.9036 - val_recall: 0.9578 - learning_rate: 6.2500e-05\n",
            "Epoch 139/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9650 - loss: 0.1738 - precision: 0.9096 - recall: 0.9408 - val_accuracy: 0.9665 - val_loss: 0.1135 - val_precision: 0.9101 - val_recall: 0.9444 - learning_rate: 3.1250e-05\n",
            "Epoch 140/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9730 - loss: 0.1542 - precision: 0.9295 - recall: 0.9486 - val_accuracy: 0.9670 - val_loss: 0.1144 - val_precision: 0.9103 - val_recall: 0.9467 - learning_rate: 3.1250e-05\n",
            "Epoch 141/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9713 - loss: 0.1552 - precision: 0.9282 - recall: 0.9408 - val_accuracy: 0.9670 - val_loss: 0.1147 - val_precision: 0.9068 - val_recall: 0.9511 - learning_rate: 3.1250e-05\n",
            "Epoch 142/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9693 - loss: 0.1593 - precision: 0.9288 - recall: 0.9383 - val_accuracy: 0.9670 - val_loss: 0.1134 - val_precision: 0.9120 - val_recall: 0.9444 - learning_rate: 3.1250e-05\n",
            "Epoch 143/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9661 - loss: 0.1617 - precision: 0.9098 - recall: 0.9403 - val_accuracy: 0.9675 - val_loss: 0.1136 - val_precision: 0.9087 - val_recall: 0.9511 - learning_rate: 3.1250e-05\n",
            "Epoch 144/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9674 - loss: 0.1623 - precision: 0.9156 - recall: 0.9406 - val_accuracy: 0.9675 - val_loss: 0.1143 - val_precision: 0.9070 - val_recall: 0.9533 - learning_rate: 3.1250e-05\n",
            "Epoch 145/200\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9661 - loss: 0.1605 - precision: 0.9078 - recall: 0.9450 - val_accuracy: 0.9675 - val_loss: 0.1135 - val_precision: 0.9087 - val_recall: 0.9511 - learning_rate: 3.1250e-05\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "🎯 REALISTIC MODEL EVALUATION\n",
            "==================================================\n",
            "✅ Accuracy:  0.9740 (Target: 0.85-0.95)\n",
            "🎯 Precision: 0.9401 (Target: 0.80-0.90)\n",
            "📊 Recall:    0.9443 (Target: 0.75-0.90)\n",
            "📈 F1-Score:  0.9422 (Target: 0.80-0.90)\n",
            "📈 AUC Score: 0.9980 (Target: 0.85-0.95)\n",
            "\n",
            "Confusion Matrix:\n",
            "TN: 1524, FP: 27\n",
            "FN: 25, TP: 424\n",
            "\n",
            "💼 Business Impact:\n",
            "False Positive Rate: 0.0174 (Target: <0.10)\n",
            "False Negative Rate: 0.0557 (Target: <0.15)\n",
            "\n",
            "✅ Improved model saved!\n"
          ]
        }
      ]
    }
  ]
}